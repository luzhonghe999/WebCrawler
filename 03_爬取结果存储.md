# 03 爬取结果存储

爬取结果的存储主要有以下几种方式：

* 简单的直接print出来
* 存到文本文件中，如CSV等
* 存入数据库

## 01 保存为csv

保存为CSV，可以先将要保存的数据存在一个list里面，通过pandas,把list转为dataframe，然后利用pandas，存为csv。

```python
from urllib import request # 引用urllib中的request
from bs4 import BeautifulSoup
import re
import pandas as pd
from pandas import DataFrame
if __name__ == "__main__":
    response = request.urlopen("http://58921.com") #获取网址请求返回值
    html =str(response.read(), encoding='utf-8') # 返回bytes转为utf8
    soup = BeautifulSoup(html, 'lxml')
    all=soup.find_all(attrs={'id': 'front_block_top_day'})[0].find('tbody').find_all('tr')
    # for film in all:
    #     print(film.a.attrs['title'])
    all_film=[]
    for film in all:
        args = film.find_all('td')
        film_info=[args[0].get_text(),
        re.findall(r'\d+\.?\d+',args[1].get_text())[0],
        re.findall(r'\D',args[1].get_text())[-1],
        re.findall(r'\d+\.?\d+',args[2].get_text())[0],
        re.findall(r'\D',args[2].get_text())[-1]] # 每一个电影的信息组成一个list
        all_film.append(film_info) # 加到总的list中
    df=DataFrame(all_film) # list转为dataframe
    df.columns = ['film_name', 'y_box_office', 'unit1', 'a_box_office', 'unit2'] # 修改列名
    df.to_csv("d:/test.csv",index=False) # 保存为csv
```

可以得到如下结果

film_name | y_box_office | unit1 | a_box_office | unit2
| - | :-: | :-: | :-: | :-: |
红海行动 | 1.41 | 亿 | 23.55 | 亿
唐人街探案2|8525.97|万|27.73|亿
捉妖记2|3444.24|万|20.65|亿
熊出没之变形记|1088.5|万|5.48|亿
西游记女儿国|831.86|万|6.89|亿

## 02 存入数据库

存入数据库一般包含以下步骤：

> 1、连接数据库
>
> 2、生成插入语句
>
> 3、执行插入语句并提交

或者利用
